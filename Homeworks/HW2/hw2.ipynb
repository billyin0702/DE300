{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import necessary libraries for the following purposes\n",
    "# 1. Read CSV from S3\n",
    "# 2. Sklearn and binary classificaiton models for data analysis\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "### Define the function to read CSV from S3\n",
    "AAKI = 'ASIAYAAO5HRMMBHYRDON'\n",
    "ASAK = '8095Atiarv6PFHjHwM12KaXRanXcRbJ2+0l59qvC'\n",
    "AST = \"IQoJb3JpZ2luX2VjEMr//////////wEaCXVzLWVhc3QtMiJHMEUCICZ66zZu2l24Ut8R8t3Zn37GpGV6GxZKvCzkJ8L8uWChAiEA/lGRv0BbplHtAkHmuF12eqlrppqwGPRaGK8ao5/EPVQq6wIIMxAAGgw1NDk3ODcwOTAwMDgiDNzxYq1zo1d/tH9aiSrIApV3LEPPlNuLt7dKD4ZadW6cMvVu9ILP9VZmHDiyfQzR1YcZGStEkbWyBEkCfB/0hugEBLJV7tfYcsE9of9Oq4ifYhZVjGLmx6c8uE3WGombLfFPH9e2rihVMcMPCs3QGQOmsyI4CUHXjeWp7/mn4lLey9dLBsVN6s+KRm36RjPuGGz5JYEJrrM6VX2zdMUoD/PVBA/N/3luqlCn1SVtIMWQuR11+bUOw43UMSb0B5zCUaqBJ/+gXyP+Ts+BqRNlTE3TMhNHBLdQNzpAXZQdgJCwZKbvNOElqcCdZYOx2Ag6JBPSQcylSwYPUfuxKnBJ8dDVlhtzlzuUHUH/HJcZ6cxPFPKnJvi3XFuILD5vNU5mpVhHJMPji92VoJaxxDn+fzN2gCNFPi48lDgx8FEDiwJkSxvits/IoZtbl3t/qb93pM6vdXHbINswjLb5sQY6pwHKUFOECjIBv9RnNxOXyDVm4WCvFznyvNTyeodsac+WJbWmnVavGqZbZ9Jb/E7+s1QfXF5lbSUsEc0lm7HiDWHPGRLGDCOpfdIiiIAPEO/UpE4Mf6EEMHugLucFXAgXU6GyHY5X5lzyM8cyvgPWRMJUla1bvr+mO7g6Qg7FnhLExb1w3FkrJb5m6tXWfxtkzlNXQkro9mh7/RqkQubKEiAPCk0oNUyGNg==\"\n",
    "bucket = \"de300spring2024\"\n",
    "key = \"bill_yin/heart_disease.csv\"\n",
    "cleaned_key = \"bill_yin/heart_disease_cleaned.csv\"\n",
    "\n",
    "s3 = boto3.client('s3', aws_access_key_id=AAKI, aws_secret_access_key=ASAK, aws_session_token=AST)\n",
    "obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "data = pd.read_csv(obj['Body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Understanding the columns\n",
    "Each column in the dataset is describe below with their respective information:\n",
    "\n",
    "| Variable  | Description                                         | Values                                                         |\n",
    "|-----------|-----------------------------------------------------|----------------------------------------------------------------|\n",
    "| age       | Age of the patient                                  | age in years                                                   |\n",
    "| sex       | Sex of the patient                                  | 1: male; 0: female                                             |\n",
    "| painloc   | Chest pain location                                 | 1: substernal, 0: otherwise                                    |\n",
    "| painexer  | Chest pain during exercise                          | 1: provoked by external exertion, 0: otherwise                 |\n",
    "| cp        | Chest pain type                                     | 1: typical angina, 2: atypical angina, 3: non-anginal pain, 4: asymptomatic |\n",
    "| trestbps  | Resting blood pressure                              | mmHg                                                           |\n",
    "| smoke     | Smoking status                                      | 1: smoker, 0: otherwise                                        |\n",
    "| fbs       | Fasting blood sugar > 120 mg/dl                     | 1: true, 0: false                                              |\n",
    "| prop      | Beta blocker used during exercise ECG               | 1: used, 0: not used                                           |\n",
    "| nitr      | Nitrates used during exercise ECG                   | 1: used, 0: not used                                           |\n",
    "| pro       | Calcium channel blocker used during exercise ECG    | 1: used, 0: not used                                           |\n",
    "| diuretic  | Diuretic used during exercise ECG                   | 1: used, 0: not used                                           |\n",
    "| thaldur   | Duration of exercise test                           | minutes                                                        |\n",
    "| thalach   | Maximum heart rate achieved                         | bpm                                                            |\n",
    "| exang     | Exercise induced angina                             | 1: yes, 0: no                                                  |\n",
    "| oldpeak   | ST depression induced by exercise relative to rest  | mm                                                             |\n",
    "| slope     | Slope of the peak exercise ST segment               | 1: upsloping, 2: flat, 3: downsloping                          |\n",
    "| target    | Diagnosis of heart disease                          | 1: heart disease, 0: no heart disease                          |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to use\n",
    "columns = ['age', 'sex', 'painloc', 'painexer', 'cp', 'trestbps', 'smoke', 'fbs', 'prop', 'nitr', \n",
    "           'pro', 'diuretic', 'thaldur', 'thalach', 'exang', 'oldpeak', 'slope', 'target']\n",
    "\n",
    "# Filter the data for wanted columns\n",
    "data = data[columns]\n",
    "\n",
    "# Validity filtering\n",
    "# Function to check if any value in a row contains spaces or is non-numeric\n",
    "def is_invalid(row):\n",
    "    for item in row:\n",
    "        if isinstance(item, str) and (' ' in item or not item.replace('.', '', 1).isdigit()):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# Filter out rows with invalid data\n",
    "data = data[~data.apply(is_invalid, axis=1)]\n",
    "\n",
    "# Remove rows that have more than 70% of the columns missing\n",
    "data = data.dropna(thresh=0.7*len(columns))\n",
    "\n",
    "# Turn age from string to int if it is not a NaN, then turn NaNs to modes\n",
    "data['age'] = data['age'].apply(lambda x: int(x) if not pd.isnull(x) else x)\n",
    "data['age'] = data['age'].fillna(data['age'].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This filtering step is similar to that in HW1, which is to remove the bad rows that are poorly formatted and cause issues with parsing. Any columns with excessive missing values (more than 70%) will be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              age         sex     painloc    painexer          cp    trestbps  \\\n",
      "count  840.000000  840.000000  560.000000  560.000000  840.000000  837.000000   \n",
      "mean    53.058333    0.779762    0.923214    0.601786    3.253571  132.044205   \n",
      "std      9.410778    0.414654    0.266489    0.489968    0.929576   19.160970   \n",
      "min     28.000000    0.000000    0.000000    0.000000    1.000000    0.000000   \n",
      "25%     46.000000    1.000000    1.000000    0.000000    3.000000  120.000000   \n",
      "50%     54.000000    1.000000    1.000000    1.000000    4.000000  130.000000   \n",
      "75%     60.000000    1.000000    1.000000    1.000000    4.000000  140.000000   \n",
      "max     77.000000    1.000000    1.000000    1.000000    4.000000  200.000000   \n",
      "\n",
      "            smoke         fbs        prop        nitr         pro    diuretic  \\\n",
      "count  175.000000  750.000000  833.000000  834.000000  836.000000  817.000000   \n",
      "mean     0.502857    0.150667    0.283313    0.266187    0.172249    0.112607   \n",
      "std      0.501427    0.357962    0.870965    0.442228    0.377823    0.316306   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "50%      1.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "75%      1.000000    0.000000    1.000000    1.000000    0.000000    0.000000   \n",
      "max      1.000000    1.000000   22.000000    1.000000    1.000000    1.000000   \n",
      "\n",
      "          thaldur     thalach       exang     oldpeak       slope      target  \n",
      "count  839.000000  839.000000  839.000000  832.000000  588.000000  840.000000  \n",
      "mean     8.672110  137.382598    0.390942    0.864423    1.765306    0.542857  \n",
      "std      3.747617   25.965375    0.488252    1.075147    0.622700    0.498457  \n",
      "min      1.000000   60.000000    0.000000   -2.600000    0.000000    0.000000  \n",
      "25%      6.000000  120.000000    0.000000    0.000000    1.000000    0.000000  \n",
      "50%      8.200000  140.000000    0.000000    0.500000    2.000000    1.000000  \n",
      "75%     10.500000  157.000000    1.000000    1.500000    2.000000    1.000000  \n",
      "max     24.000000  202.000000    1.000000    6.200000    3.000000    1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Examine the data and find their statistics\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the data into S3 and download it again\n",
    "data.to_csv('cleaned_data.csv', index=False)\n",
    "s3.upload_file('cleaned_data.csv', bucket, cleaned_key)\n",
    "\n",
    "obj = s3.get_object(Bucket=bucket, Key=cleaned_key)\n",
    "data = pd.read_csv(obj['Body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age           0\n",
      "sex           0\n",
      "painloc     280\n",
      "painexer    280\n",
      "cp            0\n",
      "trestbps      3\n",
      "smoke       665\n",
      "fbs          90\n",
      "prop          7\n",
      "nitr          6\n",
      "pro           4\n",
      "diuretic     23\n",
      "thaldur       1\n",
      "thalach       1\n",
      "exang         1\n",
      "oldpeak       8\n",
      "slope       252\n",
      "target        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Visualize the number of NAs in each column\n",
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 840\n",
      "NAs review\n",
      "age         0\n",
      "sex         0\n",
      "painloc     0\n",
      "painexer    0\n",
      "cp          0\n",
      "trestbps    0\n",
      "smoke       0\n",
      "fbs         0\n",
      "prop        0\n",
      "nitr        0\n",
      "pro         0\n",
      "diuretic    0\n",
      "thaldur     0\n",
      "thalach     0\n",
      "exang       0\n",
      "oldpeak     0\n",
      "slope       0\n",
      "target      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Clean the data\n",
    "# 1. Replace painloc and painexer NAs with 0 (default value)\n",
    "data['painloc'] = data['painloc'].fillna(0)\n",
    "data['painexer'] = data['painexer'].fillna(0)\n",
    "\n",
    "# 2. CP is a categorical variable, remove all NA rows\n",
    "data = data.dropna(subset=['cp'])\n",
    "\n",
    "# 3. Trestbps is a continuous variable, replace NAs and <= 100 with's the mode\n",
    "data['trestbps'] = data['trestbps'].apply(lambda x: data['trestbps'].mode()[0] if x <= 100 else x)\n",
    "data['trestbps'] = data['trestbps'].fillna(data['trestbps'].mode()[0])\n",
    "\n",
    "# 4. Clean the smoke column\n",
    "data = clean_smoke(data)\n",
    "\n",
    "# 5. Replace fbs, prop, nitr, pro, diuretic NAs and values greater than one with 0\n",
    "data['fbs'] = data['fbs'].apply(lambda x: 0 if x > 1 else x)\n",
    "data['fbs'] = data['fbs'].fillna(0)\n",
    "data['prop'] = data['prop'].apply(lambda x: 0 if x > 1 else x)\n",
    "data['prop'] = data['prop'].fillna(0)\n",
    "data['nitr'] = data['nitr'].apply(lambda x: 0 if x > 1 else x)\n",
    "data['nitr'] = data['nitr'].fillna(0)\n",
    "data['pro'] = data['pro'].apply(lambda x: 0 if x > 1 else x)\n",
    "data['pro'] = data['pro'].fillna(0)\n",
    "data['diuretic'] = data['diuretic'].apply(lambda x: 0 if x > 1 else x)\n",
    "data['diuretic'] = data['diuretic'].fillna(0)\n",
    "\n",
    "# 6. Thaldur and Thalach are continuous variables, replace NAs with the mean\n",
    "data['thaldur'] = data['thaldur'].fillna(data['thaldur'].mean())\n",
    "data['thalach'] = data['thalach'].fillna(data['thalach'].mean())\n",
    "\n",
    "# 7. Exang is a binary variable, replace NAs with 0\n",
    "data['exang'] = data['exang'].fillna(0)\n",
    "\n",
    "# 8. Oldpeak is a continuous variable, replace NAs, larger or equal to 4, less or equal to 0 with the mean\n",
    "data['oldpeak'] = data['oldpeak'].apply(lambda x: data['oldpeak'].mean() if x >= 4 or x <= 0 else x)\n",
    "data['oldpeak'] = data['oldpeak'].fillna(data['oldpeak'].mean())\n",
    "\n",
    "# 9. Slope is a categorical variable, replace NAs with the mode\n",
    "data['slope'] = data['slope'].fillna(data['slope'].mode()[0])\n",
    "\n",
    "# 10. Review all columns to ensure no NAs\n",
    "print(f\"Number of rows: {len(data)}\")\n",
    "print(\"NAs review\")\n",
    "print(data.isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Data Cleaning\n",
    "All boolean columns are converted into default values, which is 0. This is done to avoid any prior assumptions, as teh default values emcompass all cases that is not true.\n",
    "\n",
    "For cp and slope, their processing is done differently. Since cp has a low number of NAs, any rows that have NAs in cp will be removed. For slope, the NAs will be replaced with the mode of the column as their are more than 250 rows. Removing those rows can compromise the size of the dataset.\n",
    "\n",
    "Lastly, all continuous variables will be standarized to that columns' mode. This is to ensure that what ever is most frequent remains the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further augmentaion\n",
    "# Turn cp into a one-hot encoding\n",
    "data = pd.get_dummies(data, columns=['cp'])\n",
    "\n",
    "# Trun slope into a one-hot encoding\n",
    "data = pd.get_dummies(data, columns=['slope'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Parsing Categorical Variables\n",
    "The categorical variables are parsed using one-hot encoding. This is done to ensure that the model does not assume any ordinal relationship between the categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modeling\n",
    "The following modeling task with use Sklearn.\n",
    "\n",
    "## NOTE\n",
    "It is important to note that everytime the code is run, the results will change. This is due to the random nature of the train_test_split function and how the hyperparameters are tested and whether they converge or not. The results presented here are the best results obtained from the code from the latest evaluation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split data training/testing into 90/10\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.drop('target', axis=1), data['target'], test_size=0.1, random_state=42, stratify=data['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Different models testing\n",
    "In the following code blocks, we will be using the listed models to evaluate the dataset:\n",
    "1. Logistic Regression Classifier\n",
    "2. Random Forest Classifier\n",
    "3. Gradient Boosting Classifier\n",
    "4. Support Vector Classifier\n",
    "\n",
    "Each model will be trained and tested using the same dataset and the same train/test split. The models will be evaluated using the following metrics:\n",
    "1. Accuracy\n",
    "2. Precision\n",
    "3. Recall\n",
    "4. F1 Score\n",
    "\n",
    "To evaluate the models based on different parameters, a built in function from Sklearn, GridSearchCV will be used to calculate the metrics. All possible combinations of the hyperparameters will be tested to find the best model using this function, and the corresponding _best model_ will be used to evaluate the dataset. Its metrics will also be displayed.\n",
    "\n",
    "It is important to note that the **best** model will be the one with highest testing five-fold cross-validation accuracy, then the best model from this evaluation will be used to evaluate the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zihanyin/Documents/01-College/02-Northwestern/01-Courses/04-Senior/03-Spring/02-DE300/DE300/.venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/zihanyin/Documents/01-College/02-Northwestern/01-Courses/04-Senior/03-Spring/02-DE300/DE300/.venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/zihanyin/Documents/01-College/02-Northwestern/01-Courses/04-Senior/03-Spring/02-DE300/DE300/.venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/zihanyin/Documents/01-College/02-Northwestern/01-Courses/04-Senior/03-Spring/02-DE300/DE300/.venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/zihanyin/Documents/01-College/02-Northwestern/01-Courses/04-Senior/03-Spring/02-DE300/DE300/.venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/zihanyin/Documents/01-College/02-Northwestern/01-Courses/04-Senior/03-Spring/02-DE300/DE300/.venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/zihanyin/Documents/01-College/02-Northwestern/01-Courses/04-Senior/03-Spring/02-DE300/DE300/.venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/zihanyin/Documents/01-College/02-Northwestern/01-Courses/04-Senior/03-Spring/02-DE300/DE300/.venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/Users/zihanyin/Documents/01-College/02-Northwestern/01-Courses/04-Senior/03-Spring/02-DE300/DE300/.venv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:547: FitFailedWarning: \n",
      "15 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "15 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/zihanyin/Documents/01-College/02-Northwestern/01-Courses/04-Senior/03-Spring/02-DE300/DE300/.venv/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 895, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/zihanyin/Documents/01-College/02-Northwestern/01-Courses/04-Senior/03-Spring/02-DE300/DE300/.venv/lib/python3.9/site-packages/sklearn/base.py\", line 1474, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/Users/zihanyin/Documents/01-College/02-Northwestern/01-Courses/04-Senior/03-Spring/02-DE300/DE300/.venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 1172, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"/Users/zihanyin/Documents/01-College/02-Northwestern/01-Courses/04-Senior/03-Spring/02-DE300/DE300/.venv/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py\", line 67, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or None penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/zihanyin/Documents/01-College/02-Northwestern/01-Courses/04-Senior/03-Spring/02-DE300/DE300/.venv/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1051: UserWarning: One or more of the test scores are non-finite: [0.80823458 0.80956779 0.79898048        nan 0.81620774 0.81090972\n",
      " 0.81489195        nan 0.81091844 0.80826943 0.80826943        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 1, 'max_iter': 1000, 'multi_class': 'auto', 'penalty': 'l2', 'solver': 'liblinear'}\n",
      "Best Score is 0.8162077378877658\n",
      "########################## Cross-Validation Evaluation ##########################\n",
      "Cross-Validation Accuracy: 0.8161375661375662\n",
      "Confusion Matrix:\n",
      "[[274  72]\n",
      " [ 67 343]]\n",
      "Confusion Matrix (as percentages):\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0    36.243386      9.52381\n",
      "Actual 1     8.862434     45.37037\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.79      0.80       346\n",
      "         1.0       0.83      0.84      0.83       410\n",
      "\n",
      "    accuracy                           0.82       756\n",
      "   macro avg       0.82      0.81      0.81       756\n",
      "weighted avg       0.82      0.82      0.82       756\n",
      "\n",
      "########################## Test Set Evaluation ##########################\n",
      "Test Set Accuracy: 0.7976190476190477\n",
      "Confusion Matrix:\n",
      "[[26 12]\n",
      " [ 5 41]]\n",
      "Confusion Matrix (as percentages):\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0    30.952381    14.285714\n",
      "Actual 1     5.952381    48.809524\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.68      0.75        38\n",
      "         1.0       0.77      0.89      0.83        46\n",
      "\n",
      "    accuracy                           0.80        84\n",
      "   macro avg       0.81      0.79      0.79        84\n",
      "weighted avg       0.80      0.80      0.79        84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Logistic Regression Classifier\n",
    "params_lr = {\n",
    "    'C': [0.1, 1, 10],           # Narrow down the range to middle values\n",
    "    'penalty': ['l2', 'l1'],           # 'l1' is generally less used and not compatible with all solvers\n",
    "    'solver': ['liblinear', 'lbfgs'],  # Focus on the most commonly effective solvers\n",
    "    'multi_class': ['auto'],     # Let the model decide based on the dataset\n",
    "    'max_iter': [1000]            # Increase the max_iter to ensure convergence\n",
    "}\n",
    "\n",
    "clf_lr = LogisticRegression()\n",
    "find_best_model(clf_lr, params_lr, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Logistic Regression Classifier\n",
    "After reading the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html), the following hyperparameters seem useful:\n",
    "1. C (float): inverse of regularization strength\n",
    "2. penalty ('l1' or 'l2'): penalty type used\n",
    "3. sovler ('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'): algorithm used in optimization problem\n",
    "4. multi_class ('ovr', 'multinomial', 'auto'): type of multi-class classification\n",
    "5. max_iter (int): maximum number of iterations taken for the solvers to converge\n",
    "\n",
    "From the results above, the best model had the following hyperparameters:\n",
    "| Parameter   | Value     | Note     |\n",
    "|-------------|-----------|----------|\n",
    "| C           | 0.1       |          |\n",
    "| Max_iter    | 1000      |          |\n",
    "| Multi_class | auto      | default  |\n",
    "| Penalty     | l2        | default  |\n",
    "| Solver      | lbgfgs    |          |\n",
    "\n",
    "As seen, most of them were default value, which means the default hyperparameters are the best for this model. For max_iter the default value is 100, but the best model had 1000, which means the default value is not enough for the model to converge.\n",
    "\n",
    "With C = 0.1, the amount of regularization is also higher for this set of hyperparameters. This means that the model is less likely to overfit the data.\n",
    "\n",
    "However, given the wide disparity between the cross-validation accuracy and the test accuracy, it is possible that the model is overfitting the data. \n",
    "\n",
    "**5-CV Accuracy**: 0.819\n",
    "\n",
    "**Test Accuracy**: 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'bootstrap': True, 'max_depth': None, 'max_leaf_nodes': None, 'min_samples_leaf': 4, 'n_estimators': 100}\n",
      "Best Score is 0.8056291390728477\n",
      "########################## Cross-Validation Evaluation ##########################\n",
      "Cross-Validation Accuracy: 0.8015873015873016\n",
      "Confusion Matrix:\n",
      "[[262  84]\n",
      " [ 66 344]]\n",
      "Confusion Matrix (as percentages):\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0    34.656085    11.111111\n",
      "Actual 1     8.730159    45.502646\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.76      0.78       346\n",
      "         1.0       0.80      0.84      0.82       410\n",
      "\n",
      "    accuracy                           0.80       756\n",
      "   macro avg       0.80      0.80      0.80       756\n",
      "weighted avg       0.80      0.80      0.80       756\n",
      "\n",
      "########################## Test Set Evaluation ##########################\n",
      "Test Set Accuracy: 0.8214285714285714\n",
      "Confusion Matrix:\n",
      "[[27 11]\n",
      " [ 4 42]]\n",
      "Confusion Matrix (as percentages):\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0    32.142857    13.095238\n",
      "Actual 1     4.761905    50.000000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      0.71      0.78        38\n",
      "         1.0       0.79      0.91      0.85        46\n",
      "\n",
      "    accuracy                           0.82        84\n",
      "   macro avg       0.83      0.81      0.82        84\n",
      "weighted avg       0.83      0.82      0.82        84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Random Forest Classifier\n",
    "# Hyperparameter grid\n",
    "params_rf = {\n",
    "    'n_estimators': [100, 300],  # Focus on fewer but more impactful options\n",
    "    'max_depth': [None, 20],     # Test only unlimited and a moderately deep tree\n",
    "    'bootstrap': [True],         # Bootstrap typically provides better results\n",
    "    'min_samples_leaf': [1, 2, 4],  # Test the extremes to see the effect of this parameter\n",
    "    'max_leaf_nodes': [None, 50] # Limiting the choice to unrestricted and a reasonable limit\n",
    "}\n",
    "\n",
    "clf_rf = RandomForestClassifier()\n",
    "find_best_model(clf_rf, params_rf, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Random Forest Classifier\n",
    "After reading the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html), the following hyperparameters seem useful:\n",
    "1. n_estimators (int): number of trees in the forest\n",
    "2. max_depth (int): maximum depth of the tree\n",
    "3. bootstrap (bool): whether bootstrap samples are used when building trees\n",
    "4. min_samples_split (int): minimum number of samples required to split an internal node\n",
    "5. max_leaf_nodes (int): maximum number of samples required to be at a leaf node\n",
    "\n",
    "From the results above, the best model had the following hyperparameters:\n",
    "| Parameter         | Value     | Note     |\n",
    "|-------------------|-----------|----------|\n",
    "| Bootstrap         | True      | default  |\n",
    "| Max_depth         | None      | default  |\n",
    "| Max_leaf_nodes    | 50        |          |\n",
    "| Min_samples_leaf  | 4         |          |\n",
    "| N_estimators      | 100       | default  |\n",
    "\n",
    "Similar to the Logistic Regression Classifier, the Random Forest Classifier also had most of default hyperparameters as the best.\n",
    "\n",
    "Max_leaf_nodes = 50 and min_samples_leaf = 4 are the only hyperparameter that is not default. With their values, the model is less likely to overfit the data as the leaf nodes is limited to 50 and the minimum samples required to be at a leaf node is 4.\n",
    "\n",
    "Since one of the bottlenecks of Random Forest is the potential of overfitting (albeit less likely comapred to decision tree as it has a majority voting system), the hyperparameters set in the best model are useful to prevent this.\n",
    "\n",
    "**5-CV Accuracy**: 0.803\n",
    "\n",
    "**Test Accuracy**: 0.738"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Best Score is 0.8135500174276752\n",
      "########################## Cross-Validation Evaluation ##########################\n",
      "Cross-Validation Accuracy: 0.8134920634920635\n",
      "Confusion Matrix:\n",
      "[[254  92]\n",
      " [ 49 361]]\n",
      "Confusion Matrix (as percentages):\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0    33.597884    12.169312\n",
      "Actual 1     6.481481    47.751323\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.73      0.78       346\n",
      "         1.0       0.80      0.88      0.84       410\n",
      "\n",
      "    accuracy                           0.81       756\n",
      "   macro avg       0.82      0.81      0.81       756\n",
      "weighted avg       0.82      0.81      0.81       756\n",
      "\n",
      "########################## Test Set Evaluation ##########################\n",
      "Test Set Accuracy: 0.7976190476190477\n",
      "Confusion Matrix:\n",
      "[[24 14]\n",
      " [ 3 43]]\n",
      "Confusion Matrix (as percentages):\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0    28.571429    16.666667\n",
      "Actual 1     3.571429    51.190476\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.89      0.63      0.74        38\n",
      "         1.0       0.75      0.93      0.83        46\n",
      "\n",
      "    accuracy                           0.80        84\n",
      "   macro avg       0.82      0.78      0.79        84\n",
      "weighted avg       0.82      0.80      0.79        84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Support Vector Machine Classifier\n",
    "params_svc = {\n",
    "    'C': [1, 10],  # Focusing on a narrower, effective range\n",
    "    'kernel': ['rbf', 'linear', 'sigmoid'],  # Most commonly used and generally effective\n",
    "    'gamma': ['scale', 'auto']  # Automatic adaptation to feature scale\n",
    "}\n",
    "clf_svc = SVC()\n",
    "find_best_model(clf_svc, params_svc, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Support Vector Classifier\n",
    "After reading the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html), the following hyperparameters seem useful:\n",
    "\n",
    "1. C (float): penalty parameter of the error term\n",
    "2. kernel ('linear', 'poly', 'rbf', 'sigmoid', 'precomputed'): kernel type used in the algorithm\n",
    "3. gamma ('scale', 'auto', float): kernel coefficient for 'rbf', 'poly', 'sigmoid'\n",
    "\n",
    "From the results above, the best model had the following hyperparameters:\n",
    "| Parameter |  Value   |   Note   |\n",
    "|-----------|----------|----------|\n",
    "| C         | 10       |          |\n",
    "| Gamma     | scale    | default  |\n",
    "| Kernel    | linear   |          |\n",
    "\n",
    "With this model using a linear kernel, the model is less likely to overfit the data. This is because the linear kernel is less complex compared to the other kernels.\n",
    "\n",
    "However, the C value is 10, which means with even less regularization, the model will not overfit the data. This can either be attributed to the quality of the cleaned data or a good model. Further testing and parameter tuning is required to determine this.\n",
    "\n",
    "**5-CV Accuracy**: 0.819\n",
    "\n",
    "**Test Accuracy**: 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'criterion': 'gini', 'max_depth': 10, 'max_features': 20, 'max_leaf_nodes': 50, 'min_samples_leaf': 1, 'min_samples_split': 4}\n",
      "Best Score is 0.7843848030672708\n",
      "########################## Cross-Validation Evaluation ##########################\n",
      "Cross-Validation Accuracy: 0.75\n",
      "Confusion Matrix:\n",
      "[[252  94]\n",
      " [ 95 315]]\n",
      "Confusion Matrix (as percentages):\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0    33.333333    12.433862\n",
      "Actual 1    12.566138    41.666667\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.73      0.73      0.73       346\n",
      "         1.0       0.77      0.77      0.77       410\n",
      "\n",
      "    accuracy                           0.75       756\n",
      "   macro avg       0.75      0.75      0.75       756\n",
      "weighted avg       0.75      0.75      0.75       756\n",
      "\n",
      "########################## Test Set Evaluation ##########################\n",
      "Test Set Accuracy: 0.8214285714285714\n",
      "Confusion Matrix:\n",
      "[[30  8]\n",
      " [ 7 39]]\n",
      "Confusion Matrix (as percentages):\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0    35.714286     9.523810\n",
      "Actual 1     8.333333    46.428571\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.79      0.80        38\n",
      "         1.0       0.83      0.85      0.84        46\n",
      "\n",
      "    accuracy                           0.82        84\n",
      "   macro avg       0.82      0.82      0.82        84\n",
      "weighted avg       0.82      0.82      0.82        84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Decision Tree Classifier\n",
    "params_dt = {\n",
    "    'max_depth': [None, 10],     # Focus on either no limit or a practical depth\n",
    "    'min_samples_split': [2, 4, 10],  # Test the minimum and a slightly higher value\n",
    "    'min_samples_leaf': [1, 4],  # Small and moderate leaves\n",
    "    'max_features': [10, 20, 100],    # Auto generally performs well\n",
    "    'max_leaf_nodes': [None, 50],  # Unrestricted and a moderate cap\n",
    "    'criterion': ['gini']        # Focus on Gini which is generally faster and as effective\n",
    "}\n",
    "clf_dt = DecisionTreeClassifier()\n",
    "find_best_model(clf_dt, params_dt, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Decision Tree Classifier\n",
    "After reading the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), the following hyperparameters seem useful:\n",
    "\n",
    "1. max_depth (int): maximum depth of the tree\n",
    "2. min_samples_split (int): minimum number of samples required to split an internal node\n",
    "3. min_samples_leaf (int): minimum number of samples required to be at a leaf node\n",
    "4. max_features ('auto', 'sqrt', 'log2', None): number of features to consider when looking for the best split\n",
    "5. criterion ('gini', 'entropy'): function to measure the quality of a split\n",
    "\n",
    "Interestingly, the decision tree classifer's important hyperparameters are similar to the Random Forest Classifier. This is not surprising as Random Forest is an ensemble of Decision Trees.\n",
    "\n",
    "From the results above, the best model had the following hyperparameters:\n",
    "| Parameter         | Value     | Note     |\n",
    "|-------------------|-----------|----------|\n",
    "| Criterion         | gini      | default  |\n",
    "| Max_depth         | 10        |          |\n",
    "| Max_features      | 20        |          |\n",
    "| Max_leaf_nodes    | 50        |          |\n",
    "| Min_samples_leaf  | 1         | default  |\n",
    "| Min_samples_split | 4         | default  |\n",
    "\n",
    "The Decision Tree Classifier has a few hyperparameters that are different from the Random Forest Classifier. The most notable difference is the max_features, which is set to 20. This means that the model will consider all the features when looking for the best split, without repeating anyone too many times. This is useful as it will prevent the model from overfitting the data.\n",
    "\n",
    "Features such as max_depth, max_features, and max_leaf_nodes are all here to prevent the model from overfitting the data.\n",
    "\n",
    "**5-CV Accuracy**: 0.771\n",
    "\n",
    "**Test Accuracy**: 0.738"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'algorithm': 'ball_tree', 'leaf_size': 30, 'n_neighbors': 10, 'p': 1, 'weights': 'distance'}\n",
      "Best Score is 0.7195887068665041\n",
      "########################## Cross-Validation Evaluation ##########################\n",
      "Cross-Validation Accuracy: 0.7195767195767195\n",
      "Confusion Matrix:\n",
      "[[236 110]\n",
      " [102 308]]\n",
      "Confusion Matrix (as percentages):\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0    31.216931    14.550265\n",
      "Actual 1    13.492063    40.740741\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.68      0.69       346\n",
      "         1.0       0.74      0.75      0.74       410\n",
      "\n",
      "    accuracy                           0.72       756\n",
      "   macro avg       0.72      0.72      0.72       756\n",
      "weighted avg       0.72      0.72      0.72       756\n",
      "\n",
      "########################## Test Set Evaluation ##########################\n",
      "Test Set Accuracy: 0.7261904761904762\n",
      "Confusion Matrix:\n",
      "[[23 15]\n",
      " [ 8 38]]\n",
      "Confusion Matrix (as percentages):\n",
      "          Predicted 0  Predicted 1\n",
      "Actual 0    27.380952    17.857143\n",
      "Actual 1     9.523810    45.238095\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.61      0.67        38\n",
      "         1.0       0.72      0.83      0.77        46\n",
      "\n",
      "    accuracy                           0.73        84\n",
      "   macro avg       0.73      0.72      0.72        84\n",
      "weighted avg       0.73      0.73      0.72        84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### K-Nearest Neighbors Classifier\n",
    "params_knn = {\n",
    "    'n_neighbors': [5, 10],      # Test a moderate number of neighbors\n",
    "    'weights': ['uniform', 'distance'],  # Still important to compare these two\n",
    "    'algorithm': ['ball_tree', 'kd_tree', 'brute'],       # Let the algorithm choose the best method\n",
    "    'leaf_size': [30, 60],        # Test a couple of reasonable sizes\n",
    "    'p': [1, 2]                     # Focus on Euclidean distance which is most common\n",
    "}\n",
    "clf_knn = KNeighborsClassifier()\n",
    "find_best_model(clf_knn, params_knn, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 KNN Classifier\n",
    "After reading the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), the following hyperparameters seem useful:\n",
    "\n",
    "1. n_neighbors (int): number of neighbors to use\n",
    "2. weights ('uniform', 'distance'): weight function used in prediction\n",
    "3. algorithm ('auto', 'ball_tree', 'kd_tree', 'brute'): algorithm used to compute the nearest neighbors\n",
    "4. leaf_size (int): leaf size passed to BallTree or KDTree\n",
    "5. p (int): power parameter for the Minkowski metric (1 for manhattan, 2 for euclidean)\n",
    "\n",
    "From the results above, the best model had the following hyperparameters:\n",
    "| Parameter   | Value     | Note    |\n",
    "|-------------|-----------|---------|\n",
    "| Algorithm   | ball_tree |         |\n",
    "| Leaf_size   | 30        | default |\n",
    "| N_neighbors | 10        |         | \n",
    "| p           | 1         |         |\n",
    "| Weights     | uniform   | default |\n",
    "\n",
    "N_neighbors being larger than default is useful as it will prevent the model from overfitting the data. A notable hyperparameter here is the algorihm parameter, where ball_tree is used. Ball_tree tends to work well in higher dimensional data and in datasets that naturally cluster, which intuitively makes sense in a healthcare dataset.\n",
    "\n",
    "**5-CV Accuracy**: 0.721\n",
    "\n",
    "**Test Accuracy**: 0.679"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Classifier Conclusion\n",
    "Below is a table that comparees the models based on their 5-CV and testing accuracy:\n",
    "\n",
    "| Model                   | 5-CV Accuracy | Test Accuracy |\n",
    "|-------------------------|---------------|---------------|\n",
    "| Logistic Regression     | 0.819         | 0.75          |\n",
    "| Random Forest           | 0.803         | 0.738         |\n",
    "| Support Vector          | 0.819         | 0.75          |\n",
    "| Decision Tree           | 0.771         | 0.738         |\n",
    "| KNN                     | 0.721         | 0.679         |\n",
    "\n",
    "From the table above, the Logistic Regression and Support Vector Classifier have the highest 5-CV accuracy. However, it is important to note that not all hyperparameters combinations were tested. The data cleaning process could also be improved along with increasing data size to improve the model's performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
