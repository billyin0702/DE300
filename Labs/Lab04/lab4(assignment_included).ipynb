{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fedd0184",
   "metadata": {},
   "source": [
    "# Loading required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "afbfdafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "#required for reading .xml files\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "#required for navigating machine's directory\n",
    "import glob\n",
    "import os.path\n",
    "\n",
    "#required for communicating with SQL database\n",
    "from sqlalchemy import create_engine\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a8f6af",
   "metadata": {},
   "source": [
    "# Setup - Global Variable Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e2df180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the postgresql address for SQL base coonection\n",
    "conn_string = 'postgresql://admin:de300SPRING2024@dd300spring2024.549787090008.us-east-2.redshift-serverless.amazonaws.com:5439/dev'\n",
    "\n",
    "# new connection config for mysql locally in a docker container (due to redshift not working from TA)\n",
    "# Database connection parameters\n",
    "user = 'de300_lab04'  # Your database username\n",
    "password = 'de300u1springaccess'  # Your database password\n",
    "host = 'lab4_sql_container'  # Database host (Docker container name if on the same Docker network)\n",
    "database = 'cars_lab4'  # Database name\n",
    "\n",
    "table_name = 'car_data'  # Name of the table to create/insert data into\n",
    "pq_table_name = \"car_data_pq\"  # Name of the table to create/insert data into\n",
    "\n",
    "# Create the SQLAlchemy engine\n",
    "engine = create_engine(f'mysql+mysqlconnector://{user}:{password}@{host}/{database}')\n",
    "\n",
    "# Define global variables\n",
    "columns = ['car_model','year_of_manufacture','price', 'fuel']\n",
    "folder = \"data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12401535",
   "metadata": {},
   "source": [
    "### Utility function for writing data into the SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "05c7e906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def insert_to_table(data: pd.DataFrame, table_name:str):\n",
    "#     db = create_engine(conn_string)\n",
    "#     conn = db.connect()\n",
    "#     data.to_sql(table_name, conn, if_exists=\"replace\", index=False)\n",
    "#     conn.close()\n",
    "\n",
    "\n",
    "# Insert data to table using MySQL Docker\n",
    "def insert_to_table(data: pd.DataFrame, table_name:str):\n",
    "    data.to_sql(name=table_name, con=engine, if_exists='replace', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee295b4d",
   "metadata": {},
   "source": [
    "# Step one : Extract data from ./data/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d928b34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/used_car_prices1.csv\n",
      "./data/used_car_prices2.csv\n",
      "./data/used_car_prices3.csv\n",
      "./data/used_car_prices1.json\n",
      "./data/used_car_prices3.xml\n",
      "./data/used_car_prices2.xml\n",
      "./data/used_car_prices3.json\n",
      "./data/used_car_prices1.xml\n",
      "./data/used_car_prices2.json\n"
     ]
    }
   ],
   "source": [
    "# See all files in the data folder\n",
    "all_files = glob.glob('./data/*')\n",
    "\n",
    "# Output the list of files\n",
    "for file in all_files:\n",
    "    print(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798d2a39",
   "metadata": {},
   "source": [
    "### Function to extract data from one .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d7837037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_csv(file_to_process: str) -> pd.DataFrame:\n",
    "    \n",
    "    # add you line here to read the .csv file and return dataframe\n",
    "    # Given the file name, parse in the csv, return the dataframe\n",
    "    \n",
    "    return pd.read_csv(file_to_process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b5be00",
   "metadata": {},
   "source": [
    "### Function to extract data from one .json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6fc7e59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_json(file_to_process: str) -> pd.DataFrame:\n",
    "    \n",
    "    # add you line here to read the .json file and return dataframe\n",
    "    # Given the json file name, parse in the file line by line, as each line is a json object, and return the dataframe\n",
    "    \n",
    "    return pd.read_json(file_to_process, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810ffb74",
   "metadata": {},
   "source": [
    "### Function to extract data from one  .xml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3c0c08d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_xml(file_to_process: str) -> pd.DataFrame:\n",
    "    dataframe = pd.DataFrame(columns = columns)\n",
    "    tree = ET.parse(file_to_process)\n",
    "    root = tree.getroot()\n",
    "    for person in root:\n",
    "        car_model = person.find(\"car_model\").text\n",
    "        year_of_manufacture = int(person.find(\"year_of_manufacture\").text)\n",
    "        price = float(person.find(\"price\").text)\n",
    "        fuel = person.find(\"fuel\").text\n",
    "        sample = pd.DataFrame({\"car_model\":car_model, \"year_of_manufacture\":year_of_manufacture, \"price\":price, \"fuel\":fuel}, index = [0])\n",
    "        dataframe = pd.concat([dataframe, sample], ignore_index=True)\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10537c6b",
   "metadata": {},
   "source": [
    "### Function to extract data from the ./data/ folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "19cb67f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to extract data from all files in the data folder\n",
    "def extract() -> pd.DataFrame:\n",
    "    extracted_data = pd.DataFrame(columns = columns)\n",
    "    #for csv files\n",
    "    for csv_file in glob.glob(os.path.join(folder, \"*.csv\")):\n",
    "        extracted_data = pd.concat([extracted_data, extract_from_csv(csv_file)], ignore_index=True)\n",
    "        print(f\"Extracted data from {csv_file}\")\n",
    "\n",
    "    #add lines for json files\n",
    "    for json_file in glob.glob(os.path.join(folder, \"*.json\")):\n",
    "        extracted_data = pd.concat([extracted_data, extract_from_json(json_file)], ignore_index=True)\n",
    "        print(f\"Extracted data from {json_file}\")\n",
    "    \n",
    "    #add lines for xml files\n",
    "    for xml_file in glob.glob(os.path.join(folder, \"*.xml\")):\n",
    "        extracted_data = pd.concat([extracted_data, extract_from_xml(xml_file)], ignore_index=True)\n",
    "        print(f\"Extracted data from {xml_file}\")\n",
    "    \n",
    "    return extracted_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8a192e",
   "metadata": {},
   "source": [
    "### Extract the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a120192e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted data from data/used_car_prices1.csv\n",
      "Extracted data from data/used_car_prices2.csv\n",
      "Extracted data from data/used_car_prices3.csv\n",
      "Extracted data from data/used_car_prices1.json\n",
      "Extracted data from data/used_car_prices3.json\n",
      "Extracted data from data/used_car_prices2.json\n",
      "Extracted data from data/used_car_prices3.xml\n",
      "Extracted data from data/used_car_prices2.xml\n",
      "Extracted data from data/used_car_prices1.xml\n",
      "car_model\n",
      "year_of_manufacture\n",
      "price\n",
      "fuel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_163/1797487327.py:6: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  extracted_data = pd.concat([extracted_data, extract_from_csv(csv_file)], ignore_index=True)\n",
      "/tmp/ipykernel_163/2138107605.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe, sample], ignore_index=True)\n",
      "/tmp/ipykernel_163/2138107605.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe, sample], ignore_index=True)\n",
      "/tmp/ipykernel_163/2138107605.py:11: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe, sample], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "def main():\n",
    "    data = extract()\n",
    "    insert_to_table(data, table_name) # table name is defined as a global variable\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = main()\n",
    "\n",
    "# Test connecting to the database and querying the first 3 rows\n",
    "result = pd.read_sql(f\"SELECT * FROM {table_name} LIMIT 3\", engine)\n",
    "for row in result:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b2892f",
   "metadata": {},
   "source": [
    "# Step Two: Transformation of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8734a880",
   "metadata": {},
   "outputs": [],
   "source": [
    "staging_file = \"cars.parquet\"\n",
    "staging_data_dir = \"staging_data\"\n",
    "\n",
    "def transform(df):\n",
    "    \"\"\"_summary_\n",
    "    \n",
    "    Notes:\n",
    "        Engine is a globally defined variable\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): dataframe of the data\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_sql_query(f'SELECT * FROM {table_name}', engine)\n",
    "    print(f\"Shape of data initially: {df.shape}\")\n",
    "\n",
    "    # truncate price with 2 decimal place (add your code below)\n",
    "    df['price'] = df['price'].apply(lambda x: round(x, 2))\n",
    "    print(f\"Example of price after truncating: {df['price'].head()}\")\n",
    "\n",
    "    # remove samples with same car_model (add your code below)\n",
    "    df = df.drop_duplicates(subset='car_model')\n",
    "    print(f\"Shape of data after removing same car model: {df.shape}\")\n",
    "    \n",
    "    # Check if the directory exists\n",
    "    if not os.path.exists(staging_data_dir):\n",
    "        os.makedirs(staging_data_dir)  # Create the directory if it does not exist\n",
    "\n",
    "    # write to parquet\n",
    "    df.to_parquet(os.path.join(staging_data_dir, staging_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a8c445de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data initially: (90, 4)\n",
      "Example of price after truncating: 0     5000.00\n",
      "1     7089.55\n",
      "2    10820.90\n",
      "3     4253.73\n",
      "4     6865.67\n",
      "Name: price, dtype: float64\n",
      "Shape of data after removing same car model: (25, 4)\n",
      "  car_model  year_of_manufacture     price    fuel\n",
      "0      ritz                 2014   5000.00  Petrol\n",
      "1       sx4                 2013   7089.55  Diesel\n",
      "2      ciaz                 2017  10820.90  Petrol\n",
      "3   wagon r                 2011   4253.73  Petrol\n",
      "4     swift                 2014   6865.67  Diesel\n"
     ]
    }
   ],
   "source": [
    "# Test Parquet Transformation\n",
    "transform(data)\n",
    "\n",
    "# Read the parquet file\n",
    "pq_data = pd.read_parquet(os.path.join(staging_data_dir, staging_file))\n",
    "print(pq_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9159be5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  car_model year_of_manufacture         price    fuel\n",
      "0      ritz                2014   5000.000000  Petrol\n",
      "1       sx4                2013   7089.552239  Diesel\n",
      "2      ciaz                2017  10820.895522  Petrol\n",
      "3   wagon r                2011   4253.731343  Petrol\n",
      "4     swift                2014   6865.671642  Diesel\n"
     ]
    }
   ],
   "source": [
    "# Print the head of your data for verification\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d995c7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Step Three : Loading data for further modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c20ccef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 4)\n"
     ]
    }
   ],
   "source": [
    "# read from the .parquet file\n",
    "\n",
    "def load() -> pd.DataFrame:\n",
    "    data = pd.DataFrame()\n",
    "    for parquet_file in glob.glob(os.path.join(staging_data_dir, \"*.parquet\")):\n",
    "        data = pd.concat([pd.read_parquet(parquet_file),data])\n",
    "\n",
    "    insert_to_table(data, pq_table_name)\n",
    "\n",
    "    return data\n",
    "\n",
    "data = load()\n",
    "print(data.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
